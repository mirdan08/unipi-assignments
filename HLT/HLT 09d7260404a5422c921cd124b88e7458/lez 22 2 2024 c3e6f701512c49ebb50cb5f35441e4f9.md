# lez. 22/2/2024

# regular expressions

[ ] servono a specificare una serie di valori possibili per un certo carattere

possiamo specificae lettere sia maiuscole che miniscole ed anche numeri

[A-Z] indica un range per le lettere maisucole [a-z] per quelle miniuscole e [0-9] per i numeri, tutte queste rigaudano solo un carattere

[^….] indica caratteri che NON vogliamo matchare

possiamo usare le disguiunzione persceficare set alternativi di caratteri che vogliamo matchare possiamo anche usarli per comporre sotto pattern

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled.png)

con ? possiamo specificare se il carattere successivo è opzionale o meno , * inidica che ci può essere nessuna, una o più volte , + indica che almeno una volta o più compare , il . indica che ci può essere qualsiasi cosa 

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%201.png)

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%202.png)

altri esempi

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%203.png)

abbiamo sia do falsi positivi (sono matchate ma non le vogliamo) o falsi negativi

abbimoa sempre questo tipo di errori , possiamo cercare di ridurre i flsi positivi  (migliora accuracy) o incrementare il ccoverate (minimizzare i falsi negativi)

possiamo suare le regex anche epr efettaure sostituzioni, basta ggiungere una s/regex/sostituzione

esempio

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%204.png)

ovviamente possiamo catturare gruppi di parole , specificando con (), specificando \1 indichiamo il gruppo di nostro interesse coem sopra prende il primo meathc vedi sotto per avere più gruppi

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%205.png)

ne lcaso in qui non vogliamo catturare un gruppo ma solo specificarlo basst aggungere ?: dopo la parentesi aperta 

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%206.png)

Il corpus è una collezione curata di testi creata per uno specifico scopo, può essere testo scritto o una trascrizione del linguaggio naturale

ci sono vari criteri di selezione per il corpora

- generalita
    - sublanguage : linguaggi o specifico
    - generale: tanti aspetti linguistici messi insieme senza un bias preciso
- modealità
    - scritto
    - paralto
    - misto fra i primi 2
    - campioni di linguaggio in forma di segnale acustico
- cronologia
    - indica il linguaggio presente in uno specifico periodo temporale
- linguaggio
    - monolingua: un singolo linguaggio
    - multilingue: da più linguaggi
- integrità
    - può contenere interi testi o porzioni predefinite di lunghezza variabile

….

abbiamo i tipi per indicare un elemento del vocabolario mentre il token è una sua istanza nel testo in questione

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%207.png)

idealmente la dimensione del vocaboario cresce con il numero di token secodno la leegge di hedran  sopra illustrata

per controllare quanto è grande il testo rispetto ai token usiamo il type-token ration = $N/|V|$

il meotod più semplice è separare in  base a spazi bianchi abbiamo vari comandi per la shell o da python

alcune lingue come il cinese non usano spazi per separare le parole in questi casi dobbiamo adottare altri criteri, dobbbiamo tenere conto dei dati possiamo usare la subword tokenization

un tokenizaer ha ddue parti il learner che cerca d iapprendere dal corpus per ottenere un vocabolario mentre il segmenter cerca di prendere dei dati di test e cerca di tokenizarli in accordo col vocabolario prodotto dal segmentatore

iniziamo dan set di caratteri individuali , 

- scegli due simboli più frequenti e adiacenti nel corpus
- li mergiamo e otteniamo un nuovo simbolo
- rimpiaziamo tutte i casi adiacenti nel corpus

![Untitled](lez%2022%202%202024%20c3e6f701512c49ebb50cb5f35441e4f9/Untitled%208.png)

possiamo normalizzare le parole in foramt standard ad esempio levando punteggiatore con notazioni speciali , parole maiuscole o minuscole e verbi speciali

lemmatizzazione è la riduzione di aprole ini un lemma cio è una parola viene ridotta alla sua “radice”

un morphema è una piccola unita cche forma le parole gli stems formano le parole e gli affixed che aggiungo agli stems significati ulteriori.

alcune parole possono avere anche gli stessi lemma ma comunque significati diversi

un’analisi naive dell’analisi tiene conto solo delle radici delle parole ignorando  il loro significato

la segmentazione delle frasi è molto complicata ad esempio ! e ? sono non ambigue ma . lo è molto , possiamo usare un l’algoritmo tokenize fist : usa regole o ML per capire se è parte di una parola o fine di uan frase,