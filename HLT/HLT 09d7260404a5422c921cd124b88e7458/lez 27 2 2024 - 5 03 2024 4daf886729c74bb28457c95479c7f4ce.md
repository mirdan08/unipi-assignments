# lez. 27/2/2024 - 5/03/2024

# N-gram language models

i have to assign, based on my task a probability to a certain word in a sentence

but i dont know how to assign said probability, but lettere or words are not iid in a sentence or word so we can take adavantage of it and usedd cond probability to calculate a word given the others a mdel that computes this kind of probability is called a **Language Model**

We have to also calucalte the joint probability of words , gien the fact the we have a cond probability we can refactor it using the chain rule for con probabilities

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled.png)

we can use a more formal expression 

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%201.png)

problem: we have a really high number of possiblities e.g.

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%202.png)

Too many combinations!

We can take advantage of Markov’s assumptions: we look only at a certain number of words (e.g.)

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%203.png)

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%204.png)

The simplest case is one of indipendent words, using such a model of course would impliy random words with no real meaning, we do several extractions and estimate a a probability from these extractions

i a bigram model we estimate the prob w.r.t. on the preceding words

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%205.png)

the sentences generated from this model make more sense if looked at locally and not considering the entire sentence

now we can extend this idea to 4-5 gram models but in general it is not a good models language because the model has long distance dependencies but with n gra models we can get often get away with N-gram models for short sentences.

to compute the ccond prob we hav to count words

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%206.png)

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%207.png)

this is an example of of the MLE

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%208.png)

Her we see a matrix tha indicates how many times a wrods is followed by another one, this kind of matrices tends to be sparse , we can also normalize each column by diving its elements on the column by how many times the correspinding element on the row has occurred

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%209.png)

From the probabilities we can infer data if we want to do an estiamte we might apply a smoothing function

We normally use logairhtm to avoid underflow which is ……… (check book)

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%2010.png)

from a practical point of view is way easier to sum numbers

## LM evaluation

there  are two ways

## extrinsic or envivo evaluation

we use external knowledge to compare two models A nd B, 

1. we use both to solve a certain task on an external test set
2. we get and assesment measure
3. compare the models A and B

## intrinsic evaluation

extrinsic is not always possible , it is time consuming and expensive and doesn’t always generalize to pther applications

we use perplesity in this kinf of setting, it predicts the perofrmrnace of the model at predicting words

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%2011.png)

applying a LM model fives us terrible results because t doesn’t take into account the context

![Untitled](lez%2027%202%202024%20-%205%2003%202024%204daf886729c74bb28457c95479c7f4ce/Untitled%2012.png)

---

since we might have words that are not present in the dataset we have to distribute to probability even to words that have 0 in the training set to have at least the chance to count them in the test setsdasdadsad