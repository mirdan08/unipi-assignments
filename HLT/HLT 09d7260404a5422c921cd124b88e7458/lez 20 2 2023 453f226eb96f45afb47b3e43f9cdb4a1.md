# lez. 20/2/2023

# probability and python

questa lezione tratta concetti base di probabilità applicati a Python

l’idea principale è di assegnare una probabilita alla struttura linguistica, è un aspetto cruciale dell’ottimizzazione

- tokenization: assegniamo una probabilità ad un certo token:
- nel PoS tagging indica la probabilita che una certa parola abbia una significato rispetto ad un altro es bank con verb piuttosto che noun
- speech recognition: la pronuncia di due frasi più essere simile ma la probabilita che una sia quella corretta è molto più alta rispetto alle altre
- machine traslation: nella stessa frase ci possono  essere preferenze nell’uso di una parola rispetto ad un’altra per esprimere la stessa frase

## teoria della probabilita

…..(teoria base gia trattata)

un event semplice ha un solo risultato altrimenti è complesso e ne ha molti

## probabilita nel linguaggio

i Language Models modellano l’uso del linguaggio da grandi collezioni di testo, puo essere fatto in molto modi ma tutti sfruttano le probabilita condizionate (del tipo “prob. di A dato B” o in formule $P(A|B)$

La MLE è una tecnica per la stima dei parametri in un modello statistico , se una parola ha una certa frequenza possiamo essumere che venga mantenuta anche nel linugaggio usato ma non tutto il ocabolario viene stimato correttamente, se  una parola non occorre spesso viene tratta come non esistente o con prob P(…)=0 inoltre la MLE  è una sovrastima per le parole trattate come non impossibili, la seconda affermazione deriva dal numero limitato di parole usato che può alterare la stima (una parola puo occorrere tanto solo nel sottoinsieme di test considerato).

## smoothing function

per evitare questa problemtatica possiamo applicare una funzione di smoothing, prendiamo la frequenza di una parola nel corpo del testo detta f(w) possiamo poi calcolare $p_s(w)=(f(w)+1)/(|W|+|C|)$ , con questo sistema anche le parole impossibili o poco frequenti assumono rilevanza e poi dobbiamo normalizzare il valore divedendo tutto per la lunghezza del vocabolario e quella del corpo del testo.

## classificatore bayesiano

dato un training set di documenti etichettati possiamo stimare la probabiltia che undocumentosia in una certa classe

………

---